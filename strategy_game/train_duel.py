import os
from multiprocessing import freeze_support
from sb3_contrib.ppo_mask import MaskablePPO
from sb3_contrib.common.wrappers import ActionMasker
from stable_baselines3.common.vec_env import DummyVecEnv
from stable_baselines3.common.callbacks import BaseCallback

from gym_strategy.envs.StrategyEnvDuel import StrategyEnvDuel

# FunciÃ³n para aplicar la mÃ¡scara de acciones vÃ¡lidas
def mask_fn(env):
    return env._get_action_mask()

# Callback para loguear progreso durante el entrenamiento
class LogCallback(BaseCallback):
    def __init__(self, log_every=5000, verbose=1):
        super().__init__(verbose)
        self.log_every = log_every

    def _on_step(self) -> bool:
        infos = self.locals.get("infos", [])
        if infos:
            for info in infos:
                ep = info.get("episode")
                if ep and self.n_calls % self.log_every == 0:
                    r, l = ep["r"], ep["l"]
                    print(f"ðŸ“ˆ Paso: {self.n_calls}, recompensa: {r:.2f}, longitud: {l}")
        return True

if __name__ == "__main__":
    freeze_support()

    # 1) Crear el entorno
    base_env = StrategyEnvDuel()
    masked_env = ActionMasker(base_env, mask_fn)

    # 2) Vectorizar (un solo entorno para empezar)
    venv = DummyVecEnv([lambda: masked_env])

    # 3) Crear el modelo PPO con mÃ¡scara
    model = MaskablePPO(
        policy="MlpPolicy",
        env=venv,
        verbose=1,
        ent_coef=0.01,          # Incentiva menos la exploraciÃ³n aleatoria
        learning_rate=1e-4,      # Velocidad de aprendizaje estÃ¡ndar
        n_steps=4096,            # N pasos por actualizaciÃ³n
        batch_size=256,          # TamaÃ±o del batch
        clip_range=0.2,          # Rango de clip de PPO
        policy_kwargs=dict(
            net_arch=[dict(pi=[128, 128], vf=[128, 128])]  # Red mÃ¡s grande para el duelo
        ),
    )

    # 4) Entrenar el modelo
    model.learn(
        total_timesteps=2_000_000,  # ðŸš€ Entrena 2 millones de pasos (puedes cambiarlo si quieres)
        callback=LogCallback(log_every=5000),
    )

    # 5) Guardar el modelo
    model.save("ppo_duel_v2")
    print("âœ… Modelo guardado como 'ppo_duel_v2.zip'")
